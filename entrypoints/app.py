"""Aplicativo Streamlit para o RAG Chatbot.

Interface web interativa para o sistema de chatbot RAG local.
"""

import streamlit as st
import logging
from pathlib import Path
from io import BytesIO

from src.rag_chatbot.core import RAGChatbot
from src.rag_chatbot.components.loaders import UniversalLoader
from src.rag_chatbot.components.embedders import MiniLMEmbedder
from src.rag_chatbot.components.vector_stores import ChromaVectorStore
from src.rag_chatbot.components.llms import OllamaLLM
from src.rag_chatbot.components.text_splitters import RecursiveCharacterTextSplitter
from src.rag_chatbot.config import (
    DEFAULT_LLM_MODEL, 
    CHROMA_PERSIST_DIRECTORY,
    CHUNK_SIZE,
    CHUNK_OVERLAP
)

# Configurar pÃ¡gina
st.set_page_config(
    page_title="Chatbot RAG Local",
    page_icon="ğŸ¤–",
    layout="wide"
)

logger = logging.getLogger(__name__)


@st.cache_resource
def inicializar_chatbot(model_name: str = DEFAULT_LLM_MODEL):
    """Inicializa o chatbot (cached para nÃ£o recarregar).
    
    Args:
        model_name: Nome do modelo LLM a usar.
        
    Returns:
        InstÃ¢ncia configurada do RAGChatbot.
    """
    logger.info("Inicializando componentes do chatbot...")
    
    try:
        loader = UniversalLoader()
        embedder = MiniLMEmbedder()
        # Usar o diretÃ³rio de persistÃªncia do config
        store = ChromaVectorStore(collection_name="streamlit_rag")
        llm = OllamaLLM(model_name=model_name)
        
        # Adicionar text splitter para divisÃ£o inteligente de documentos
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        
        chatbot = RAGChatbot(
            loader=loader,
            embedder=embedder,
            store=store,
            llm=llm,
            text_splitter=text_splitter
        )
        
        logger.info("Chatbot inicializado com sucesso!")
        return chatbot
    
    except Exception as e:
        logger.error(f"Erro ao inicializar chatbot: {e}")
        st.error(f"Erro ao inicializar: {e}")
        return None


def main():
    """FunÃ§Ã£o principal da aplicaÃ§Ã£o Streamlit."""
    
    # TÃ­tulo principal
    st.title("ğŸ¤– Chatbot RAG Local Personalizado")
    st.markdown("*Retrieval-Augmented Generation com sua base de conhecimento privada*")
    
    # Sidebar para configuraÃ§Ã£o global
    st.sidebar.title("âš™ï¸ ConfiguraÃ§Ãµes Globais")
    
    # SeleÃ§Ã£o do modelo LLM
    model_name = st.sidebar.text_input(
        "Modelo LLM (Ollama)",
        value=DEFAULT_LLM_MODEL,
        help="Nome do modelo Ollama instalado (ex: llama3, mistral)"
    )
    
    # Caminho dos dados
    data_path = st.sidebar.text_input(
        "Caminho da Pasta de Dados",
        value="./data",
        help="Pasta contendo arquivos (.txt, .md, .pdf, .docx)"
    )
    
    # Inicializar chatbot
    chatbot = inicializar_chatbot(model_name)
    
    if chatbot is None:
        st.error("âš ï¸ NÃ£o foi possÃ­vel inicializar o chatbot. Verifique os logs.")
        st.stop()
    
    # Criar abas
    tab_chat, tab_manage, tab_help = st.tabs(["ğŸ’¬ Chat", "ğŸ“š Gerenciar RAG", "ğŸ’¡ Ajuda"])
    
    # ========== ABA: CHAT ==========
    with tab_chat:
        st.markdown("### Chat com RAG")
        st.markdown("FaÃ§a perguntas baseadas nos documentos da sua base de conhecimento.")
        
        # Upload de imagem (opcional, para modelos multimodais)
        uploaded_image = st.file_uploader(
            "ğŸ–¼ï¸ Enviar imagem (opcional, para anÃ¡lise com modelo multimodal)",
            type=["png", "jpg", "jpeg"],
            help="O modelo multimodal (ex: llava) analisarÃ¡ a imagem junto com o contexto do RAG"
        )
        
        # Mostrar preview da imagem se carregada
        if uploaded_image:
            st.image(uploaded_image, caption="Imagem enviada", width=300)
        
        # Inicializar histÃ³rico de mensagens
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # Mostrar histÃ³rico de mensagens
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                # Mostrar imagem se houver
                if "image" in message and message["image"]:
                    st.image(message["image"], caption="Imagem enviada pelo usuÃ¡rio", width=300)
                
                st.markdown(message["content"])
                
                # Mostrar fontes se disponÃ­vel
                if message["role"] == "assistant" and "sources" in message:
                    with st.expander("ğŸ“š Ver fontes utilizadas"):
                        for i, source in enumerate(message["sources"], 1):
                            st.markdown(f"**Fonte {i}:**")
                            st.markdown(f"- **Arquivo:** {source.metadata.get('source', 'N/A')}")
                            st.markdown(f"- **Caminho:** {source.metadata.get('path', 'N/A')}")
                            
                            # Mostrar informaÃ§Ãµes de chunk se disponÃ­vel
                            if 'chunk_index' in source.metadata:
                                chunk_idx = source.metadata.get('chunk_index', 0)
                                total_chunks = source.metadata.get('total_chunks', 1)
                                st.markdown(f"- **Chunk:** {chunk_idx + 1} de {total_chunks}")
                            
                            st.markdown(f"- **Trecho:** {source.content[:200]}...")
                            st.markdown("---")
        
        # Input do usuÃ¡rio
        if prompt := st.chat_input("ğŸ’¬ FaÃ§a sua pergunta..."):
            # Processar imagem se houver
            image_data = None
            image_for_display = None
            
            if uploaded_image:
                # Ler dados da imagem
                image_data = uploaded_image.read()
                # Guardar para exibiÃ§Ã£o
                uploaded_image.seek(0)
                image_for_display = uploaded_image
            
            # Adicionar mensagem do usuÃ¡rio ao histÃ³rico
            user_message = {
                "role": "user", 
                "content": prompt,
                "image": image_for_display
            }
            st.session_state.messages.append(user_message)
            
            # Exibir mensagem do usuÃ¡rio
            with st.chat_message("user"):
                if image_for_display:
                    st.image(image_for_display, caption="Imagem enviada pelo usuÃ¡rio", width=300)
                st.markdown(prompt)
            
            # Gerar e exibir resposta do assistente
            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤” Pensando... (Buscando no RAG e gerando resposta)"):
                    try:
                        # Passar histÃ³rico de chat (sem imagens para o histÃ³rico)
                        chat_history = [
                            {"role": msg["role"], "content": msg["content"]} 
                            for msg in st.session_state.messages[:-1]  # Excluir a pergunta atual
                        ]
                        
                        # Fazer pergunta com contexto conversacional e imagem (se houver)
                        response = chatbot.ask(
                            prompt, 
                            image_data=image_data,
                            chat_history=chat_history if chat_history else None
                        )
                        sources = chatbot.get_sources(prompt)
                        
                        st.markdown(response)
                        
                        # Adicionar resposta ao histÃ³rico com fontes
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": response,
                            "sources": sources,
                            "image": None
                        })
                        
                        # Mostrar fontes
                        if sources:
                            with st.expander("ğŸ“š Ver fontes utilizadas"):
                                for i, source in enumerate(sources, 1):
                                    st.markdown(f"**Fonte {i}:**")
                                    st.markdown(f"- **Arquivo:** {source.metadata.get('source', 'N/A')}")
                                    st.markdown(f"- **Caminho:** {source.metadata.get('path', 'N/A')}")
                                    st.markdown(f"- **Trecho:** {source.content[:200]}...")
                                    st.markdown("---")
                        
                    except Exception as e:
                        error_msg = f"âŒ Erro ao gerar resposta: {e}"
                        st.error(error_msg)
                        logger.error(f"Erro ao processar pergunta: {e}", exc_info=True)
    
    # ========== ABA: GERENCIAR RAG ==========
    with tab_manage:
        st.markdown("### Gerenciar Base de Conhecimento")
        
        # SeÃ§Ã£o de ingestÃ£o
        st.markdown("#### ğŸ“¥ Alimentar RAG com Documentos")
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            st.info(f"ğŸ“ Pasta atual: **{data_path}**")
            
            if Path(data_path).exists():
                # Buscar arquivos de todos os formatos suportados
                files = (
                    list(Path(data_path).glob("*.txt")) + 
                    list(Path(data_path).glob("*.md")) +
                    list(Path(data_path).glob("*.pdf")) +
                    list(Path(data_path).glob("*.docx"))
                )
                if files:
                    st.success(f"âœ… {len(files)} arquivo(s) encontrado(s)")
                    with st.expander("Ver arquivos"):
                        for file in files:
                            st.text(f"- {file.name} ({file.suffix})")
                else:
                    st.warning("âš ï¸ Nenhum arquivo suportado (.txt, .md, .pdf, .docx) encontrado na pasta")
            else:
                st.error(f"âŒ Pasta nÃ£o encontrada: {data_path}")
        
        with col2:
            if st.button("ğŸ”„ Alimentar RAG", type="primary", use_container_width=True):
                if not Path(data_path).exists():
                    st.error(f"âŒ Pasta nÃ£o encontrada: {data_path}")
                else:
                    with st.spinner("ğŸ“– Lendo, processando e vetorizando documentos..."):
                        try:
                            num_docs = chatbot.ingest_data(data_path)
                            
                            if num_docs > 0:
                                st.success(f"âœ… RAG alimentado com sucesso!")
                                st.info(f"ğŸ“„ {num_docs} chunk(s) processado(s)")
                                st.info(f"ğŸ“ Fonte: {data_path}")
                                st.info(f"ğŸ’¾ Persistido em: {CHROMA_PERSIST_DIRECTORY}")
                                st.info(f"âœ‚ï¸ DivisÃ£o inteligente: {CHUNK_SIZE} chars, overlap {CHUNK_OVERLAP}")
                            else:
                                st.warning("âš ï¸ Nenhum documento suportado encontrado na pasta.")
                                
                        except Exception as e:
                            st.error(f"âŒ Falha na ingestÃ£o: {e}")
                            logger.error(f"Erro durante ingestÃ£o: {e}", exc_info=True)
        
        st.markdown("---")
        
        # SeÃ§Ã£o de inspeÃ§Ã£o
        st.markdown("#### ğŸ” Inspecionar Fontes")
        st.markdown("Veja quais fontes seriam recuperadas para uma pergunta, sem chamar o LLM.")
        
        inspect_query = st.text_input("Digite uma pergunta para inspecionar:", key="inspect_query")
        inspect_k = st.slider("NÃºmero de fontes a recuperar:", min_value=1, max_value=10, value=3, key="inspect_k")
        
        if st.button("ğŸ” Buscar Fontes", type="secondary"):
            if inspect_query:
                with st.spinner("Buscando fontes..."):
                    try:
                        sources = chatbot.get_sources(inspect_query, k=inspect_k)
                        
                        if sources:
                            st.success(f"âœ… Encontradas {len(sources)} fonte(s)")
                            
                            for i, source in enumerate(sources, 1):
                                with st.container():
                                    st.markdown(f"### ğŸ“„ Fonte {i}")
                                    st.markdown(f"**Arquivo:** {source.metadata.get('source', 'N/A')}")
                                    st.markdown(f"**Caminho:** {source.metadata.get('path', 'N/A')}")
                                    st.markdown("**ConteÃºdo:**")
                                    st.text_area(
                                        f"ConteÃºdo da fonte {i}",
                                        value=source.content,
                                        height=150,
                                        key=f"source_{i}",
                                        label_visibility="collapsed"
                                    )
                                    st.markdown("---")
                        else:
                            st.warning("âš ï¸ Nenhuma fonte encontrada. Certifique-se de que o RAG foi alimentado.")
                            
                    except Exception as e:
                        st.error(f"âŒ Erro ao buscar fontes: {e}")
                        logger.error(f"Erro ao inspecionar fontes: {e}", exc_info=True)
            else:
                st.warning("Por favor, digite uma pergunta para inspecionar.")
    
    # ========== ABA: AJUDA ==========
    with tab_help:
        st.markdown("### ğŸ’¡ Como Usar o Chatbot RAG")
        
        st.markdown("""
        #### ğŸ“– Passo a Passo
        
        1. **Prepare seus documentos**
           - Adicione arquivos `.txt`, `.md`, `.pdf` ou `.docx` na pasta de dados (padrÃ£o: `./data/`)
           - Os documentos devem conter informaÃ§Ãµes que vocÃª quer consultar
           - **Novidade v3.0:** Suporte para PDFs e DOCX!
        
        2. **Alimente o RAG**
           - VÃ¡ para a aba **"ğŸ“š Gerenciar RAG"**
           - Clique em **"ğŸ”„ Alimentar RAG"**
           - Aguarde o processamento dos documentos
           - **Novidade v3.0:** DivisÃ£o inteligente em chunks para melhor precisÃ£o!
        
        3. **FaÃ§a perguntas**
           - VÃ¡ para a aba **"ğŸ’¬ Chat"**
           - Digite sua pergunta no chat
           - **Novidade v3.0:** Envie imagens junto com sua pergunta (ex: grÃ¡ficos, diagramas)
           - **Novidade v3.0:** FaÃ§a perguntas de acompanhamento - o chatbot lembra da conversa!
           - O sistema buscarÃ¡ informaÃ§Ãµes relevantes e gerarÃ¡ uma resposta
        
        4. **Verifique as fontes**
           - Clique em **"Ver fontes utilizadas"** abaixo de cada resposta
           - Veja quais documentos/chunks foram usados para gerar a resposta
        
        #### ğŸ–¼ï¸ AnÃ¡lise Multimodal
        
        **Novidade v3.0:** O chatbot agora pode analisar imagens!
        - Envie uma imagem usando o uploader na aba de Chat
        - O modelo multimodal (ex: llava) analisarÃ¡ a imagem junto com o contexto do RAG
        - Ideal para analisar grÃ¡ficos, diagramas, capturas de tela, etc.
        
        #### ğŸ’¬ MemÃ³ria Conversacional
        
        **Novidade v3.0:** O chatbot lembra da conversa!
        - FaÃ§a perguntas de acompanhamento sem repetir o contexto
        - Exemplo: "E o segundo ponto que vocÃª mencionou?" funciona!
        - O histÃ³rico Ã© preservado durante toda a sessÃ£o
        
        #### ğŸ” InspeÃ§Ã£o de Fontes
        
        Use a seÃ§Ã£o **"ğŸ” Inspecionar Fontes"** na aba **"Gerenciar RAG"** para:
        - Testar quais documentos/chunks seriam recuperados para uma pergunta
        - Verificar se o RAG estÃ¡ funcionando corretamente
        - Entender a qualidade da busca semÃ¢ntica
        - **Novidade v3.0:** Veja informaÃ§Ãµes sobre chunks (Ã­ndice e total)
        
        #### âš™ï¸ ConfiguraÃ§Ãµes
        
        - **Modelo LLM**: Altere na sidebar (padrÃ£o: llama3)
        - **Modelo Multimodal**: Configurado como llava (para anÃ¡lise de imagens)
        - **Pasta de Dados**: Altere na sidebar (padrÃ£o: ./data)
        - **Chunk Size**: {CHUNK_SIZE} caracteres por chunk
        - **Chunk Overlap**: {CHUNK_OVERLAP} caracteres de sobreposiÃ§Ã£o
        - **PersistÃªncia**: Os dados sÃ£o salvos automaticamente em `{CHROMA_PERSIST_DIRECTORY}`
        
        #### ğŸ“‹ Requisitos
        
        - **Ollama** instalado e rodando
        - Modelo de texto baixado: `ollama pull llama3` (ou outro modelo)
        - **Opcional:** Modelo multimodal para imagens: `ollama pull llava`
        - Documentos na pasta de dados (suporta .txt, .md, .pdf, .docx)
        
        #### ğŸ› Troubleshooting
        
        - Se o chatbot nÃ£o responder, verifique se o Ollama estÃ¡ rodando: `ollama serve`
        - Se nenhuma fonte for encontrada, certifique-se de ter alimentado o RAG
        - Para usar anÃ¡lise de imagens, certifique-se de ter o modelo llava instalado
        - Verifique os logs em `./logs/rag_chatbot.log` para detalhes
        
        #### ğŸ†• Novidades v3.0
        
        - âœ‚ï¸ **DivisÃ£o inteligente de documentos** em chunks para maior precisÃ£o
        - ğŸ“„ **Suporte multi-formato:** PDFs e DOCX alÃ©m de TXT e MD
        - ğŸ–¼ï¸ **AnÃ¡lise multimodal:** Envie imagens junto com perguntas
        - ğŸ’¬ **MemÃ³ria conversacional:** Chatbot lembra do contexto da conversa
        - ğŸ¯ **Rastreamento de chunks:** Veja de qual parte do documento vem cada resposta
        """)
        
        st.markdown("---")
        st.markdown("**ğŸ’¾ PersistÃªncia:** Os dados do RAG sÃ£o salvos automaticamente e persistem entre reinicializaÃ§Ãµes.")
        st.markdown(f"**ğŸ“ DiretÃ³rio ChromaDB:** `{CHROMA_PERSIST_DIRECTORY}`")
    
    # BotÃ£o para limpar histÃ³rico na sidebar
    st.sidebar.markdown("---")
    if st.sidebar.button("ğŸ—‘ï¸ Limpar HistÃ³rico do Chat"):
        st.session_state.messages = []
        st.rerun()


if __name__ == "__main__":
    main()

# ü§ñ Chatbot RAG Local Personalizado

Sistema de Chatbot RAG (Retrieval-Augmented Generation) modular, extens√≠vel e test√°vel em Python, com interface Streamlit.

## üìã Vis√£o Geral

Este projeto implementa um chatbot que responde perguntas baseadas em uma base de conhecimento privada (documentos locais), utilizando:

- **RAG (Retrieval-Augmented Generation)**: Combina busca sem√¢ntica com gera√ß√£o de texto
- **LLM Local**: Usa Ollama para rodar modelos de linguagem localmente
- **Embeddings**: SentenceTransformers para vetoriza√ß√£o de texto
- **Vector Store**: ChromaDB para armazenamento e busca eficiente
- **UI**: Interface web interativa com Streamlit

### Princ√≠pios de Design (SOLID)

O sistema foi constru√≠do seguindo os princ√≠pios SOLID:

- **[S] Single Responsibility**: Cada classe tem uma responsabilidade √∫nica
- **[O] Open/Closed**: Aberto para extens√£o, fechado para modifica√ß√£o
- **[L] Liskov Substitution**: Componentes s√£o intercambi√°veis via interfaces
- **[I] Interface Segregation**: Interfaces pequenas e focadas
- **[D] Dependency Inversion**: Depend√™ncia de abstra√ß√µes, n√£o implementa√ß√µes

## üèóÔ∏è Estrutura do Projeto

```
/chatbot-rag-local/
‚îÇ
‚îú‚îÄ‚îÄ /data/                   # (Ignorado) Documentos do usu√°rio
‚îú‚îÄ‚îÄ /logs/                   # (Ignorado) Arquivos de log
‚îÇ
‚îú‚îÄ‚îÄ /rag_chatbot/            # Pacote principal
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ interfaces.py        # Interfaces abstratas (ABC)
‚îÇ   ‚îú‚îÄ‚îÄ core.py              # Orquestrador RAGChatbot
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configura√ß√µes
‚îÇ   ‚îî‚îÄ‚îÄ /components/         # Implementa√ß√µes concretas
‚îÇ       ‚îú‚îÄ‚îÄ loaders.py       # FolderLoader
‚îÇ       ‚îú‚îÄ‚îÄ embedders.py     # MiniLMEmbedder
‚îÇ       ‚îú‚îÄ‚îÄ vector_stores.py # ChromaVectorStore
‚îÇ       ‚îî‚îÄ‚îÄ llms.py          # OllamaLLM, MockLLM
‚îÇ
‚îú‚îÄ‚îÄ /tests/                  # Testes
‚îÇ   ‚îú‚îÄ‚îÄ test_core.py         # Testes do RAGChatbot
‚îÇ   ‚îî‚îÄ‚îÄ test_components.py   # Testes dos componentes
‚îÇ
‚îú‚îÄ‚îÄ app.py                   # Aplicativo Streamlit
‚îú‚îÄ‚îÄ requirements.txt         # Depend√™ncias
‚îú‚îÄ‚îÄ README.md               # Esta documenta√ß√£o
‚îî‚îÄ‚îÄ .gitignore              # Arquivos ignorados
```

## üöÄ Instala√ß√£o

### 1. Clonar o Reposit√≥rio

```bash
git clone https://github.com/strondata/fastrag.git
cd fastrag
```

### 2. Criar Ambiente Virtual (Recomendado)

```bash
python -m venv .venv
source .venv/bin/activate  # No Windows: .venv\Scripts\activate
```

### 3. Instalar Depend√™ncias

```bash
pip install -r requirements.txt
```

### 4. Configurar LLM Local (Ollama)

#### Instalar Ollama

- **Linux/Mac**: 
  ```bash
  curl -fsSL https://ollama.com/install.sh | sh
  ```
- **Windows**: Baixar de [ollama.com](https://ollama.com)

#### Baixar um Modelo

```bash
ollama pull llama3
# Ou outros modelos: mistral, phi, gemma, etc.
```

#### Iniciar Ollama

```bash
ollama serve
```

## üìñ Uso

### Executar o Chatbot

```bash
streamlit run app.py
```

O aplicativo abrir√° em `http://localhost:8501`

### Alimentar a Base de Conhecimento

1. **Adicionar Documentos**: Coloque seus arquivos `.txt` ou `.md` na pasta `./data/`
   
   ```bash
   mkdir -p data
   echo "O cachorro √© marrom e amig√°vel." > data/animais.txt
   ```

2. **No Streamlit**: Clique no bot√£o **"Alimentar RAG"** na sidebar

3. **Fazer Perguntas**: Use o chat para interagir com sua base de conhecimento

### Exemplo de Uso

```
Usu√°rio: Qual a cor do cachorro?
Bot: O cachorro √© marrom.
```

## üß™ Testes

### Executar Todos os Testes

```bash
pytest
```

### Executar Testes Espec√≠ficos

```bash
# Testes do core
pytest tests/test_core.py

# Testes dos componentes
pytest tests/test_components.py

# Com verbosidade
pytest -v
```

### Executar Testes com Cobertura

```bash
# Gerar relat√≥rio de cobertura
pytest --cov=rag_chatbot

# Gerar relat√≥rio HTML detalhado
pytest --cov=rag_chatbot --cov-report=html

# O relat√≥rio HTML ser√° gerado em htmlcov/index.html
```

### Cobertura Atual

**62.70% de cobertura de c√≥digo** com 72+ testes passando!

- ‚úÖ `core.py`: 100%
- ‚úÖ `config.py`: 100%
- ‚úÖ `components/embedders.py`: 100%
- ‚úÖ `components/vector_stores.py`: 95.45%
- ‚úÖ `components/loaders.py`: 90.16%
- ‚úÖ `components/llms.py`: 83.33%

Ver [documenta√ß√£o completa de testes](docs/wiki/Testing.md) para mais detalhes.

## üìö Documenta√ß√£o Completa

### Wiki Abrangente

Criamos uma wiki completa em `docs/wiki/` com documenta√ß√£o detalhada:

- **[Home](docs/wiki/Home.md)** - Vis√£o geral do projeto
- **[Quick Start](docs/wiki/Quick-Start.md)** - Guia r√°pido de in√≠cio
- **[Architecture](docs/wiki/Architecture.md)** - Arquitetura e princ√≠pios SOLID
- **[Components](docs/wiki/Components.md)** - Refer√™ncia completa de componentes
- **[Testing](docs/wiki/Testing.md)** - Guia de testes e cobertura (62.70%)
- **[Advanced Features](docs/wiki/Advanced-Features.md)** - T√©cnicas avan√ßadas de RAG

### Recursos Destacados

- ‚úÖ **62.70% de cobertura de testes** (72+ testes passando)
- ‚úÖ **Documenta√ß√£o completa** com exemplos de c√≥digo
- ‚úÖ **Guias de arquitetura** explicando princ√≠pios SOLID
- ‚úÖ **Refer√™ncia de API** para todos os componentes
- ‚úÖ **Tutoriais avan√ßados** sobre t√©cnicas de RAG

## üê≥ Docker

### Executar com Docker Compose

A maneira mais f√°cil de rodar a aplica√ß√£o completa (incluindo Ollama):

```bash
# Iniciar todos os servi√ßos
docker-compose up -d

# Baixar modelo no container Ollama
docker exec -it fastrag-ollama ollama pull llama3

# Verificar logs
docker-compose logs -f app

# Parar servi√ßos
docker-compose down
```

A aplica√ß√£o estar√° dispon√≠vel em `http://localhost:8501`

### Construir Apenas a Aplica√ß√£o

```bash
# Construir imagem
docker build -t fastrag-app .

# Executar (assumindo Ollama j√° rodando)
docker run -p 8501:8501 \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/chroma_data:/app/chroma_data \
  -e OLLAMA_HOST=http://host.docker.internal:11434 \
  fastrag-app
```

## üîß Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto (use `.env.example` como template):

```bash
# Copiar exemplo
cp .env.example .env

# Editar conforme necess√°rio
nano .env
```

Configura√ß√µes dispon√≠veis:

- `DEFAULT_LLM_MODEL`: Modelo LLM padr√£o (default: `llama3`)
- `OLLAMA_HOST`: URL do servidor Ollama (default: `http://localhost:11434`)
- `DEFAULT_EMBEDDING_MODEL`: Modelo de embedding (default: `all-MiniLM-L6-v2`)
- `DATA_DIR`: Diret√≥rio de dados (default: `./data`)
- `LOGS_DIR`: Diret√≥rio de logs (default: `./logs`)
- `CHROMA_PERSIST_DIRECTORY`: Diret√≥rio do ChromaDB (default: `./chroma_data`)
- `DEFAULT_COLLECTION_NAME`: Nome da cole√ß√£o (default: `rag_store`)
- `DEFAULT_TOP_K`: N√∫mero de documentos a recuperar (default: `3`)
- `LOG_LEVEL`: N√≠vel de log (default: `INFO`)

### Personalizar Modelo LLM

Via vari√°vel de ambiente:

```bash
export DEFAULT_LLM_MODEL="mistral"
streamlit run app.py
```

Ou configure na interface do Streamlit.

### Personalizar Template de Prompt

```python
from rag_chatbot.core import RAGChatbot

custom_template = """
Responda em portugu√™s brasileiro:
CONTEXTO: {context}
PERGUNTA: {question}
RESPOSTA:
"""

chatbot = RAGChatbot(
    loader=loader,
    embedder=embedder,
    store=store,
    llm=llm,
    prompt_template=custom_template
)
```

### Usar Diferentes Componentes

```python
from rag_chatbot.components.loaders import FolderLoader
from rag_chatbot.components.embedders import MiniLMEmbedder
from rag_chatbot.components.vector_stores import ChromaVectorStore
from rag_chatbot.components.llms import OllamaLLM
from rag_chatbot.core import RAGChatbot

# Componentes customizados
loader = FolderLoader()
embedder = MiniLMEmbedder(model_name="paraphrase-multilingual-MiniLM-L12-v2")
store = ChromaVectorStore(collection_name="my_custom_rag")
llm = OllamaLLM(model_name="mistral")

chatbot = RAGChatbot(loader, embedder, store, llm)
```

## üìö API Reference

### RAGChatbot

```python
chatbot = RAGChatbot(loader, embedder, store, llm)

# Alimentar com dados
num_docs = chatbot.ingest_data("/path/to/data")

# Fazer pergunta
response = chatbot.ask("Sua pergunta aqui", k=3)

# Obter documentos fonte
sources = chatbot.get_sources("Pergunta", k=3)
```

### Interfaces

- `IDocumentLoader`: Carrega documentos de uma fonte
- `IEmbeddingModel`: Gera embeddings de texto
- `IVectorStore`: Armazena e busca vetores
- `ILocalLLM`: Gera texto com LLM local

## üêõ Troubleshooting

### Erro: "Pacote 'ollama' n√£o encontrado"

```bash
pip install ollama
```

### Erro: "Ollama n√£o est√° rodando"

```bash
ollama serve
```

### Erro: "Modelo n√£o encontrado"

```bash
ollama pull llama3
```

### Logs

Verifique os logs em `./logs/rag_chatbot.log` para debugging detalhado.

### Persist√™ncia de Dados

Os dados do RAG s√£o automaticamente salvos no diret√≥rio `./chroma_data/` e persistem entre reinicializa√ß√µes. Para limpar e recome√ßar:

```bash
rm -rf ./chroma_data
```

## üÜï Novidades v2.0

### Configura√ß√£o via Vari√°veis de Ambiente
- Suporte a arquivo `.env` para configura√ß√£o
- N√£o mais hardcoded - todas as configura√ß√µes s√£o customiz√°veis
- Persist√™ncia real do ChromaDB entre reinicializa√ß√µes

### Interface Melhorada
- UI reorganizada em abas: Chat, Gerenciar RAG, Ajuda
- Visualiza√ß√£o de fontes utilizadas em cada resposta
- Inspe√ß√£o de fontes sem chamar o LLM
- Melhor organiza√ß√£o da sidebar

### Docker Support
- Dockerfile pronto para produ√ß√£o
- Docker Compose com Ollama integrado
- Volumes para persist√™ncia de dados
- Network configurada para comunica√ß√£o entre servi√ßos

### Qualidade de C√≥digo
- Cobertura de testes com pytest-cov
- Documenta√ß√£o completa
- Pronto para deploy

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudan√ßas (`git commit -m 'feat: adiciona MinhaFeature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

### Padr√£o de Commits

Usamos [Conventional Commits](https://www.conventionalcommits.org/):

- `feat:` Nova funcionalidade
- `fix:` Corre√ß√£o de bug
- `docs:` Documenta√ß√£o
- `test:` Testes
- `refactor:` Refatora√ß√£o de c√≥digo

## üìÑ Licen√ßa

Este projeto √© de c√≥digo aberto.

## üë• Autores

- Desenvolvido seguindo especifica√ß√µes SOLID e boas pr√°ticas Python

## üîó Links √öteis

- [Ollama](https://ollama.com)
- [Streamlit](https://streamlit.io)
- [ChromaDB](https://www.trychroma.com)
- [SentenceTransformers](https://www.sbert.net)

---

**Nota**: Este √© um projeto educacional que demonstra implementa√ß√£o de RAG com arquitetura modular e princ√≠pios SOLID.
